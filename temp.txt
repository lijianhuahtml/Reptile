============================================要获取网页的源码==============================================
import requests

def get_html(url):
    try:
        # 发送HTTP请求获取网页内容
        response = requests.get(url)

        # 检查响应状态码
        if response.status_code == 200:
            # 返回网页的HTML内容
            return response.text
        else:
            print("Failed to retrieve the webpage. Status code:", response.status_code)
            return None
    except Exception as e:
        print("An error occurred:", e)
        return None

# 替换为你要获取源码的网页URL
url = 'https://example.com'
html_content = get_html(url)

if html_content:
    print(html_content)

==========================模拟HTTP请求： 使用选择的库发送HTTP请求，获取网页的HTML内容===========================
import requests

# 定义目标网页的URL
url = 'https://www.example.com'

# 发送HTTP GET请求
response = requests.get(url)

# 检查响应状态码
if response.status_code == 200:
    # 输出网页的HTML内容
    print(response.text)
else:
    # 打印错误信息
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")

=========================解析HTML： 使用BeautifulSoup等库解析HTML，提取出评论所在的元素=======================

// https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start 使用手册

from bs4 import BeautifulSoup
import requests

def crawl_comments(url):
    try:
        # 发送HTTP请求获取网页内容
        response = requests.get(url)

        # 检查响应状态码
        if response.status_code == 200:
            # 使用BeautifulSoup解析HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # 根据网页结构定位评论的元素，以下仅为示例，请根据实际情况修改
            comments = soup.find_all('div', class_='comment')

            # 处理评论数据，可以存储到文件或数据库，以下仅为示例，请根据实际情况修改
            for comment in comments:
                print(comment.text)
        else:
            print("Failed to retrieve the webpage. Status code:", response.status_code)
    except Exception as e:
        print("An error occurred:", e)

# 替换为你要爬取的网页URL
url = 'https://example.com'
crawl_comments(url)

============处理分页： 如果评论分布在多个页面上，需要编写代码来处理分页，递归地获取所有页面上的评论=======================

import requests
from bs4 import BeautifulSoup

def crawl_comments(url):
    try:
        while url:
            # 发送HTTP请求获取网页内容
            response = requests.get(url)

            # 检查响应状态码
            if response.status_code == 200:
                # 使用BeautifulSoup解析HTML
                soup = BeautifulSoup(response.text, 'html.parser')

                # 根据网页结构定位评论的元素，以下仅为示例，请根据实际情况修改
                comments = soup.find_all('div', class_='comment')

                # 处理评论数据，可以存储到文件或数据库，以下仅为示例，请根据实际情况修改
                for comment in comments:
                    print(comment.text)

                # 查找下一页的链接，以下仅为示例，请根据实际情况修改
                next_page_link = soup.find('a', class_='next-page')['href']

                # 构建下一页的完整链接
                url = 'https://example.com' + next_page_link if next_page_link else None
            else:
                print("Failed to retrieve the webpage. Status code:", response.status_code)
                break
    except Exception as e:
        print("An error occurred:", e)

# 替换为你要爬取的网页URL
url = 'https://example.com/first-page'
crawl_comments(url)

=======================================存储到文本文件：========================================================

import requests
from bs4 import BeautifulSoup

def crawl_comments(url):
    try:
        while url:
            # 发送HTTP请求获取网页内容
            response = requests.get(url)

            # 检查响应状态码
            if response.status_code == 200:
                # 使用BeautifulSoup解析HTML
                soup = BeautifulSoup(response.text, 'html.parser')

                # 根据网页结构定位评论的元素，以下仅为示例，请根据实际情况修改
                comments = soup.find_all('div', class_='comment')

                # 处理评论数据，存储到文本文件，以下仅为示例，请根据实际情况修改
                with open('comments.txt', 'a', encoding='utf-8') as file:
                    for comment in comments:
                        file.write(comment.text + '\n')

                # 查找下一页的链接，以下仅为示例，请根据实际情况修改
                next_page_link = soup.find('a', class_='next-page')['href']

                # 构建下一页的完整链接
                url = 'https://example.com' + next_page_link if next_page_link else None
            else:
                print("Failed to retrieve the webpage. Status code:", response.status_code)
                break
    except Exception as e:
        print("An error occurred:", e)

# 替换为你要爬取的网页URL
url = 'https://example.com/first-page'
crawl_comments(url)

===========================================================================================================